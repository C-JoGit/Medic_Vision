{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pill Recognition Classify\n",
    "\n",
    "### In this notebook, pill recognition algorithm will be established, model trained and saved for deployment to the object detection algorithm. CNN will be used at first, but due to the small dataset used for this purpose, Random Forest Classify will also be used and the best performing model adopted. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importation of the necessary librabries.\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.image import imread \n",
    "import seaborn as sns\n",
    "import glob\n",
    "import cv2\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from shutil import copyfile\n",
    "from random import seed, random\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from os import walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'medication\\\\GeloMytrol1.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-725f40eda590>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'GeloMytrol'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# load imgae pixels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;31m# plot raw pixel data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1462\u001b[0m             raise ValueError('Only know how to handle PNG; with Pillow '\n\u001b[1;32m   1463\u001b[0m                              'installed, Matplotlib can handle more images')\n\u001b[0;32m-> 1464\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1465\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpil_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1466\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_png\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2877\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2878\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2879\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'medication\\\\GeloMytrol1.jpg'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAI8AAABjCAYAAACi5VNqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAEdUlEQVR4nO3dz2tcZRTG8e9jTTfZdJGARY0oBEPcxaHEjWQjNEHoxkW7KXQTFP0DXNn/oSAtWQTppi6lSIrbuqk0EVuiIERBDBZKLURKRSkcF/ciaUgyd07eYe5Mng8MZHJ/zLm8D3dm7sucq4jALOOFQRdgw8vhsTSHx9IcHktzeCzN4bG0ruGRtCrpoaTNA5ZL0hVJW5LuS5orX6a1UZMzzxfA2UOWLwLT9WMZuHr0smwYdA1PRNwGHh+yyjngelTuAKcknS5VoLVXic88LwO/73q+Xf/PRtyLBfahff6375yHpGWqtzbGx8ffnpmZKfDydlQbGxuPImKy1+1KhGcbeHXX81eAP/ZbMSJWgBWATqcT6+vrBV7ejkrSb5ntSrxt3QQu1t+65oGdiHhQYL/Wcl3PPJJuAAvAhKRt4DIwBhAR14A1YAnYAp4Cl/pVrLVL1/BExIUuywP4uFhFNjR8hdnSHB5Lc3gszeGxNIfH0hweS3N4LM3hsTSHx9IcHktzeCzN4bE0h8fSHB5Lc3gszeGxNIfH0hweS3N4LM3hsTSHx9IcHktzeCytUXgknZX0c92D59N9li9I2pH0Q/34rHyp1jZNfjF6AvgceI/qd+l3Jd2MiJ/2rPptRLzfhxqtpZqcec4AWxHxa0T8C3xJ1ZPHjrkm4Wnaf+cdSfck3ZL0VpHqrNWatFhp0n/ne+C1iHgiaQn4iqrN3PM72tWfZ2pqqsdSrW2anHm69t+JiL8i4kn99xowJmli744iYiUiOhHRmZzsuZeQtUyT8NwFpiW9LukkcJ6qJ8//JL0kSfXfZ+r9/lm6WGuXJi1Wnkn6BPgGOAGsRsSPkj6sl18DPgA+kvQM+Bs4H76dzsjToMbYbeXaQ9JGRHR63c5XmC3N4bE0h8fSHB5Lc3gszeGxNIfH0hweS3N4LM3hsTSHx9IcHktzeCzN4bE0h8fSHB5Lc3gszeGxNIfH0hweS3N4LM3hsTSHx9JK9eeRpCv18vuS5sqXam3TNTy7+vMsArPABUmze1ZbpGpsME3VyOBq4TqthUr15zkHXI/KHeCUpNOFa7WWKdWfp2kPHxshpfrzNFnnuf48wD+SNhu8fptNAI8GXUQBb2Y2ahKerv15Gq5DRKwAKwCS1jM/rm+TUTgGqI4js12R/jz184v1t655YCciHmQKsuFRqj/PGrAEbAFPgUv9K9naYmD9eSQt129jQ2sUjgHyxzGw8Njw8/SEpfU9PKMwtTEKt0+QtCrp4UGXR1LjEBF9e1B9wP4FeAM4CdwDZvesswTcorpWNA9818+a+nQMC8DXg661y3G8C8wBmwcs73kc+n3mGYWpjZG4fUJE3AYeH7JKz+PQ7/CMwtTGcbl9Qs/j0OQK81EUm9oYoGK3T2i5nseh32eeYlMbA1Ts9gkt1/M49Ds8ozC1cVxun9DzOPT1bStGYGqj4TG0/vYJkm5QfSuckLQNXAbGID8OvsJsab7CbGkOj6U5PJbm8Fiaw2NpDo+lOTyW5vBY2n9LRRKNrMeEZAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# EDA - a sneakpeak into the files in the medication (train dataset) folder. \n",
    "\n",
    "# load dataset\n",
    "\n",
    "folder = \"medication\\\\\"\n",
    "\n",
    "# plot first few images\n",
    "\n",
    "for i in range(1, 8):\n",
    "    # define subplot\n",
    "    plt.subplot(330 + 0 + i)\n",
    "    # define filename\n",
    "    filename = folder + 'GeloMytrol' + str(i) + '.jpg'\n",
    "    # load imgae pixels\n",
    "    image = imread(filename)\n",
    "    # plot raw pixel data\n",
    "    plt.imshow(image)\n",
    "# show the figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the names of the files in the directory\n",
    "filenames = os.listdir(\"medication\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pill_encoding(folder=\"medication\\\\\"):\n",
    "    \"\"\"Convert the photos and labels from images and texts to numbers - encoding, 16 selected medications are encoded here\"\"\"\n",
    "    \"\"\"folder: specify the folder where the medication photos can be found\"\"\"\n",
    "\n",
    "    # location of train set\n",
    "    #folder = \"medication\\\\\"\n",
    "    photos, labels = list(), list()\n",
    "\n",
    "    # enumerate files in the directory\n",
    "    for file in os.listdir(folder):\n",
    "        # determine class\n",
    "        output = 0.0\n",
    "        if file.startswith(\"Bis\"):\n",
    "            output = 1.0\n",
    "        if file.startswith(\"Candecor\"):\n",
    "            output = 2.0\n",
    "        if file.startswith(\"Candes\"):\n",
    "            output = 3.0\n",
    "        if file.startswith(\"Eunova\"):\n",
    "            output = 4.0\n",
    "        if file.startswith(\"Gelo\"):\n",
    "            output = 5.0\n",
    "        if file.startswith(\"Ibu\"):\n",
    "            output = 6.0\n",
    "        if file.startswith(\"Imo\"):\n",
    "            output = 7.0\n",
    "        if file.startswith(\"Klo\"):\n",
    "            output = 8.0\n",
    "        if file.startswith(\"Lem\"):\n",
    "            output = 9.0\n",
    "        if file.startswith(\"Lor\"):\n",
    "            output = 10.0\n",
    "        if file.startswith(\"Nov\"):\n",
    "            output = 11.0\n",
    "        if file.startswith(\"Sed\"):\n",
    "            output = 12.0\n",
    "        if file.startswith(\"Tax\"):\n",
    "            output = 13.0\n",
    "        if file.startswith(\"Tho\"):\n",
    "            output = 14.0\n",
    "        if file.startswith(\"Vit\"):\n",
    "            output = 15.0\n",
    "        \n",
    "        # load image\n",
    "        photo = load_img(folder + file, target_size=(300, 300))\n",
    "\n",
    "        # convert to numpy array\n",
    "        photo = img_to_array(photo)\n",
    "\n",
    "        # store\n",
    "        photos.append(photo)\n",
    "        labels.append(output)\n",
    "\n",
    "    # convert to numpy arrays\n",
    "    photos = np.asarray(photos)\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    return photos, labels\n",
    "\n",
    "def save_encodings(filename1, filename2, data1, data2):\n",
    "    \"\"\"filename1: desired name of dataset 1, filename2: desired name of dataset 2\"\"\"\n",
    "    # save the reshaped photos\n",
    "    np.save(filename1, data1)\n",
    "    np.save(filename2, data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132, 300, 300, 3) (132,)\n"
     ]
    }
   ],
   "source": [
    "photos, labels = pill_encoding(\"medication\\\\\")\n",
    "print(photos.shape, labels.shape)\n",
    "\n",
    "save_encodings(filename1=\"trainset_photos.npy\", filename2=\"trainset_labels.npy\", data1=photos, data2=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 300, 300, 3) (17,)\n"
     ]
    }
   ],
   "source": [
    "# encode the test set\n",
    "test_photos, test_labels = pill_encoding(folder=\"medication_testset\\\\\")\n",
    "print(test_photos.shape, test_labels.shape)\n",
    "\n",
    "save_encodings(filename1=\"testset_photos.npy\", filename2=\"testset_labels.npy\", data1=test_photos, data2=test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline CNN Model\n",
    "\n",
    "The baseline model using CNN is expected to produce crappy result, given the size of the training set, random forest classify will be used in the hope of getting a better prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132, 300, 300, 3) (132,) (17, 300, 300, 3) (17,)\n"
     ]
    }
   ],
   "source": [
    "train_images = np.load(\"trainset_photos.npy\")\n",
    "train_labels = np.load(\"trainset_labels.npy\")\n",
    "test_images = np.load(\"testset_photos.npy\")\n",
    "test_labels = np.load(\"testset_labels.npy\")\n",
    "\n",
    "# Normalize the dataset\n",
    "train_images, test_images = train_images/255.0, test_images/255.0\n",
    "\n",
    "print(train_images.shape, train_labels.shape, test_images.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132, 300, 300, 3) (132, 1) (17, 300, 300, 3) (17, 1)\n"
     ]
    }
   ],
   "source": [
    "# reshape the labels\n",
    "\n",
    "train_labels = train_labels.reshape(-1,1)\n",
    "test_labels = test_labels.reshape(-1,1)\n",
    "\n",
    "print(train_images.shape, train_labels.shape, test_images.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5/5 - 5s - loss: -4.4461e+10 - accuracy: 0.0455 - val_loss: -4.3522e+21 - val_accuracy: 0.0588\n",
      "Epoch 2/20\n",
      "5/5 - 3s - loss: nan - accuracy: 0.0379 - val_loss: nan - val_accuracy: 0.0588\n",
      "Epoch 3/20\n",
      "5/5 - 4s - loss: nan - accuracy: 0.0682 - val_loss: nan - val_accuracy: 0.0588\n",
      "Epoch 4/20\n",
      "5/5 - 4s - loss: nan - accuracy: 0.0682 - val_loss: nan - val_accuracy: 0.0588\n",
      "Epoch 5/20\n",
      "5/5 - 3s - loss: nan - accuracy: 0.0682 - val_loss: nan - val_accuracy: 0.0588\n",
      "Epoch 6/20\n",
      "5/5 - 4s - loss: nan - accuracy: 0.0682 - val_loss: nan - val_accuracy: 0.0588\n",
      "Epoch 7/20\n",
      "5/5 - 3s - loss: nan - accuracy: 0.0682 - val_loss: nan - val_accuracy: 0.0588\n",
      "Epoch 8/20\n",
      "5/5 - 4s - loss: nan - accuracy: 0.0682 - val_loss: nan - val_accuracy: 0.0588\n",
      "Epoch 9/20\n",
      "5/5 - 4s - loss: nan - accuracy: 0.0682 - val_loss: nan - val_accuracy: 0.0588\n",
      "Epoch 10/20\n",
      "5/5 - 4s - loss: nan - accuracy: 0.0682 - val_loss: nan - val_accuracy: 0.0588\n",
      "Epoch 11/20\n",
      "5/5 - 4s - loss: nan - accuracy: 0.0682 - val_loss: nan - val_accuracy: 0.0588\n",
      "Epoch 12/20\n",
      "5/5 - 4s - loss: nan - accuracy: 0.0682 - val_loss: nan - val_accuracy: 0.0588\n",
      "Epoch 13/20\n",
      "5/5 - 4s - loss: nan - accuracy: 0.0682 - val_loss: nan - val_accuracy: 0.0588\n",
      "Epoch 14/20\n",
      "5/5 - 4s - loss: nan - accuracy: 0.0682 - val_loss: nan - val_accuracy: 0.0588\n",
      "Epoch 15/20\n",
      "5/5 - 4s - loss: nan - accuracy: 0.0682 - val_loss: nan - val_accuracy: 0.0588\n",
      "Epoch 16/20\n",
      "5/5 - 4s - loss: nan - accuracy: 0.0682 - val_loss: nan - val_accuracy: 0.0588\n",
      "Epoch 17/20\n",
      "5/5 - 4s - loss: nan - accuracy: 0.0682 - val_loss: nan - val_accuracy: 0.0588\n",
      "Epoch 18/20\n",
      "5/5 - 4s - loss: nan - accuracy: 0.0682 - val_loss: nan - val_accuracy: 0.0588\n",
      "Epoch 19/20\n",
      "5/5 - 4s - loss: nan - accuracy: 0.0682 - val_loss: nan - val_accuracy: 0.0588\n",
      "Epoch 20/20\n",
      "5/5 - 4s - loss: nan - accuracy: 0.0682 - val_loss: nan - val_accuracy: 0.0588\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(300, 300, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile model\n",
    "opt = SGD(lr=0.001, momentum=0.9)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# train the model\n",
    "history = model.fit(train_images, train_labels, epochs=20, verbose=2, validation_data= (test_images, test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 0s - loss: nan - accuracy: 0.0588\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.01, 0.1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05882352963089943\n"
     ]
    }
   ],
   "source": [
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 300, 300, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1_input (InputLayer)  [(None, 300, 300, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 300, 300, 32)      896       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 300, 300, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 300, 300, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300, 300, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 150, 150, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 150, 150, 64)      18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 150, 150, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 150, 150, 64)      256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 360000)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               46080128  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 46,146,593\n",
      "Trainable params: 46,146,209\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 26s 5s/step - loss: -7.3678 - accuracy: 0.0590 - val_loss: -47.4011 - val_accuracy: 0.0588\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 28s 5s/step - loss: -30.3008 - accuracy: 0.0429 - val_loss: -50.3497 - val_accuracy: 0.0588\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 29s 5s/step - loss: -42.1425 - accuracy: 0.0325 - val_loss: -52.6440 - val_accuracy: 0.0588\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 28s 5s/step - loss: -40.9352 - accuracy: 0.0412 - val_loss: -55.3143 - val_accuracy: 0.0588\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 29s 6s/step - loss: -49.3013 - accuracy: 0.0490 - val_loss: -57.7624 - val_accuracy: 0.0588\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 32s 6s/step - loss: -53.8373 - accuracy: 0.0377 - val_loss: -59.9434 - val_accuracy: 0.0588\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 33s 6s/step - loss: -55.0433 - accuracy: 0.0351 - val_loss: -62.1306 - val_accuracy: 0.0588\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 30s 6s/step - loss: -56.9504 - accuracy: 0.0369 - val_loss: -64.2245 - val_accuracy: 0.0588\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 41s 8s/step - loss: -60.3156 - accuracy: 0.0377 - val_loss: -66.3256 - val_accuracy: 0.0588\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 28s 5s/step - loss: -58.9378 - accuracy: 0.0516 - val_loss: -68.4394 - val_accuracy: 0.0588\n"
     ]
    }
   ],
   "source": [
    "# modify the features and retrain the model\n",
    "cnn_features = Sequential()\n",
    "cnn_features.add(Conv2D(32, (3, 3), activation = 'sigmoid', padding = 'same', input_shape=(300,300,3)))\n",
    "cnn_features.add(BatchNormalization())\n",
    "\n",
    "cnn_features.add(Conv2D(32, (3, 3), activation = 'sigmoid', padding = 'same', kernel_initializer = 'he_uniform'))\n",
    "cnn_features.add(BatchNormalization())\n",
    "cnn_features.add(MaxPooling2D(2, 2))\n",
    "\n",
    "cnn_features.add(Conv2D(64, (3, 3), activation = 'sigmoid', padding = 'same', kernel_initializer = 'he_uniform'))\n",
    "cnn_features.add(BatchNormalization())\n",
    "\n",
    "cnn_features.add(Conv2D(64, (3, 3), activation = 'sigmoid', padding = 'same', kernel_initializer = 'he_uniform'))\n",
    "cnn_features.add(BatchNormalization())\n",
    "cnn_features.add(MaxPooling2D(2, 2))\n",
    "\n",
    "cnn_features.add(Flatten())\n",
    "\n",
    "# Add layers for deep learning prediction\n",
    "x = cnn_features.output  \n",
    "x = Dense(128, activation = 'sigmoid', kernel_initializer = 'he_uniform')(x)\n",
    "prediction_layer = Dense(1, activation = 'softmax')(x)\n",
    "\n",
    "# Make a new model combining both feature extractor and x\n",
    "cnn_model = Model(inputs=cnn_features.input, outputs=prediction_layer)\n",
    "cnn_model.compile(optimizer='rmsprop',loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "print(cnn_model.summary()) \n",
    "\n",
    "#Train the CNN model\n",
    "history = cnn_model.fit(train_images, train_labels, epochs=10, validation_data = (test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model Using Convolutional Filters + Random Forest for image classification\n",
    "\n",
    "### We will use features from CNN to train RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = train_images, train_labels, test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.ravel()\n",
    "y_test = y_test.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the features from CNN for RF\n",
    "X_train = cnn_features.predict(X_train)\n",
    "X_test = cnn_features.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.5882352941176471\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Model\n",
    "rf_model = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "# Train the model on training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set\n",
    "y_pred= rf_model.predict(X_test)\n",
    "\n",
    "# Print overall accuracy\n",
    "print(\"Accuracy = \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, the random forest model performed better than the convolutional neural network. Next, some hyper-parameter tuning will be implemented in order to see if the accuracy of the prediction could be improved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialising hyperparameter grid\n",
    "param_grid = param_grid = {'bootstrap': [True, False],\n",
    " 'max_depth': [20, 40, 60, 80, 100, None],\n",
    " 'max_features': ['auto', 'sqrt'],\n",
    " 'min_samples_leaf': [1, 2, 4],\n",
    " 'min_samples_split': [2, 5, 10],\n",
    " 'n_estimators': [50, 100, 200, 400, 600, 800, 1000]}\n",
    "\n",
    "#GridSearch CV\n",
    "grid_search = GridSearchCV(estimator = rf_model, param_grid = param_grid, cv = 3)\n",
    "\n",
    "# Train the model on training data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print best paramaters from GridSearch\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get best performing model from GridSearch\n",
    "best = grid_search.best_estimator_\n",
    "best.fit(X_train, y_train)\n",
    "y_pred = best.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print overall accuracy\n",
    "print(\"Accuracy = \", accuracy_score(y_pred, y_test))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b680f19dc88a0e7d20231683ee8f83efcc33b9a1faee7aaf2b39dca108c216ef"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
